---
title: "Regression"
output: html_document
---

We now plot the density of the log of total earnings to see if it normally distributed. We want to find out whether it is best to regress on the log transformed variable or leave it untouched.

```{r, warning=FALSE, message=FALSE}
# what about log(salary)? 
ggplot(clean_data, aes(x=log(tot_earnings_USD)))+
  geom_density()


ggplot(clean_data, aes(x=log(tot_earnings_USD)))+
  stat_ecdf()
```

Seeing this graph, it is probably optimal to use the log of total earnings (USD) as it is normally distributed, in comparison to the untransformed version we saw before. We also remove rows with omitted variables as they are not useful in the regression.

```{r, warning=FALSE, message=FALSE}
regression_data <- clean_data %>% 
  na.omit()
```


```{r, warning=FALSE, message=FALSE}
regression_data <- regression_data[regression_data$tot_earnings_USD != 0,] %>% 
  mutate(log_earnings = log(tot_earnings_USD)) 
```

We now create a naive model, regressing on the log of incomes, with a couple of basic variables. 

```{r, warning=FALSE, message=FALSE}
model_1 <- lm(log_earnings ~ how_old_are_you + overall_years_of_professional_experience +
     years_of_experience_in_field, data = regression_data)

model_1 %>% get_regression_table()
model_1 %>% get_regression_summaries()
```

Education variable is also slightly cleaned to make things easier in the regression
```{r, warning=FALSE, message=FALSE}
regression_data$highest_level_of_education_completed <- gsub("Some college", "College degree", regression_data$highest_level_of_education_completed)
```

```{r, warning=FALSE, message=FALSE}
regression_data_dummies <- regression_data %>% 
  dummy_cols(select_columns = c("gender", "highest_level_of_education_completed", "race"))
```

```{r, warning=FALSE, message=FALSE}
names(regression_data_dummies)[names(regression_data_dummies) == "highest_level_of_education_completed_Master's degree"] <- "highest_level_of_education_completed_Masters_degree"
names(regression_data_dummies)[names(regression_data_dummies) == "highest_level_of_education_completed_College degree"] <- "highest_level_of_education_completed_College_degree"
names(regression_data_dummies)[names(regression_data_dummies) == "highest_level_of_education_completed_High School"] <- "highest_level_of_education_completed_High_School"
```


```{r, warning=FALSE, message=FALSE}
model_2 <- lm(log_earnings ~ how_old_are_you + overall_years_of_professional_experience +
     years_of_experience_in_field  + gender_Man + gender_Woman + highest_level_of_education_completed_College_degree + highest_level_of_education_completed_High_School + highest_level_of_education_completed_Masters_degree  + race_Asian + race_White + race_Hispanic + race_Black , data = regression_data_dummies)

model_2 %>% get_regression_table()
```

```{r, warning=FALSE, message=FALSE}
model_2 %>% get_regression_summaries()
```

```{r, warning=FALSE, message=FALSE}
car::vif(model_2)
```

Since overall years of professional experience has very high multicollinearity, we remove it in order to improve this
```{r, warning=FALSE, message=FALSE}
model_3 <- lm(log_earnings ~ how_old_are_you  + years_of_experience_in_field  + gender_Man + gender_Woman + highest_level_of_education_completed_College_degree + highest_level_of_education_completed_High_School + highest_level_of_education_completed_Masters_degree  + race_Asian + race_White + race_Hispanic + race_Black , data = regression_data_dummies)

model_3 %>% get_regression_table()
```

```{r, warning=FALSE, message=FALSE}
model_3 %>% get_regression_summaries()
```

It worked! No more VIF problems.
```{r, warning=FALSE, message=FALSE}
car::vif(model_3)
```

In this fourth and last model we will now remove any variables that were not significant at 5% level in model_3. Thus we remove variables such as
We will also remove some outliers (10%) to try to improve the fit of the regression, as some variables' outliers may be influencing the regression.

Here we remove the age brackets which proved insignificant in the regression.
```{r, warning=FALSE, message=FALSE}
regression_data_filtered <- regression_data_dummies %>% 
  filter(how_old_are_you == "25-34" | how_old_are_you == "35-44" | how_old_are_you == "55-64")
```

Same variables as model_3 except for race_Black which has been removed and the age values which were omitted for insignificance.

```{r, warning=FALSE, message=FALSE}
model_4 <- lm(log_earnings ~ how_old_are_you  + years_of_experience_in_field  + gender_Man + gender_Woman + highest_level_of_education_completed_College_degree + highest_level_of_education_completed_High_School + highest_level_of_education_completed_Masters_degree  + race_Asian + race_White + race_Hispanic , data = regression_data_filtered)

model_4 %>% get_regression_table()
```
```{r, warning=FALSE, message=FALSE}
model_4 %>% get_regression_summaries()
```
Here we eliminate the top and bottom 5% (outliers) from the total earnings category.

``` {r, warning=FALSE, message=FALSE}
Q <- quantile(regression_data_filtered$tot_earnings_USD, probs=c(.05, .95))

regression_no_outliers <- subset(regression_data_filtered, regression_data_filtered$tot_earnings_USD > (Q[1]) & regression_data_filtered$tot_earnings_USD < (Q[2]))
```


We also remove the age cateogry 35-44 as it was insignificant in the previous regression.
{r, warning=FALSE, message=FALSE}
regression_no_outliers <- regression_no_outliers %>% 
  filter(how_old_are_you == "25-34" | how_old_are_you == "55-64")


``` {r, warning=FALSE, message=FALSE}
model_5 <- lm(log_earnings ~ how_old_are_you  + years_of_experience_in_field  + gender_Man + gender_Woman + highest_level_of_education_completed_College_degree + highest_level_of_education_completed_High_School + highest_level_of_education_completed_Masters_degree  + race_Asian + race_White + race_Hispanic , data = regression_no_outliers)

model_5 %>% get_regression_table()
```

``` {r, warning=FALSE, message=FALSE}
model_5 %>% get_regression_summaries()
```

